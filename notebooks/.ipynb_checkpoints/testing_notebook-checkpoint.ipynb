{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86965c7d-e4f1-4b96-99f3-6a724a586cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1/.local/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/user1/.local/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN3c104cuda9SetDeviceEi'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import numpy as np\n",
    "import argparse\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.backends.cudnn as cudnn\n",
    "from utils import test_single_volume\n",
    "from importlib import import_module\n",
    "from segment_anything import sam_model_registry\n",
    "from datasets.dataset_synapse import Synapse_dataset\n",
    "from icecream import ic\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "935d4abb-2347-4030-8414-e6b0189a6549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "sam, img_embedding_size = sam_model_registry['vit_b'](image_size=512,\n",
    "                                                                    num_classes=2,\n",
    "                                                                    checkpoint='/home/manav/SAMed-mod/checkpoints/sam_vit_b_01ec64.pth', pixel_mean=[0, 0, 0],\n",
    "                                                                    pixel_std=[1, 1, 1])\n",
    "\n",
    "pkg = import_module('sam_lora_image_encoder')\n",
    "net = pkg.LoRA_Sam(sam, 4).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a70ab748-29a5-4412-9d23-75e888295cc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type:depth-idx)                                  Param #\n",
       "================================================================================\n",
       "LoRA_Sam                                                --\n",
       "├─Sam: 1-1                                              --\n",
       "│    └─ImageEncoderViT: 2-1                             786,432\n",
       "│    │    └─PatchEmbed: 3-1                             (590,592)\n",
       "│    │    └─ModuleList: 3-2                             85,261,824\n",
       "│    │    └─Sequential: 3-3                             (787,456)\n",
       "│    └─PromptEncoder: 2-2                               --\n",
       "│    │    └─PositionEmbeddingRandom: 3-4                --\n",
       "│    │    └─ModuleList: 3-5                             1,024\n",
       "│    │    └─Embedding: 3-6                              256\n",
       "│    │    └─Sequential: 3-7                             4,684\n",
       "│    │    └─Embedding: 3-8                              256\n",
       "│    └─MaskDecoder: 2-3                                 --\n",
       "│    │    └─TwoWayTransformer: 3-9                      3,291,264\n",
       "│    │    └─Embedding: 3-10                             256\n",
       "│    │    └─Embedding: 3-11                             768\n",
       "│    │    └─Sequential: 3-12                            73,952\n",
       "│    │    └─ModuleList: 3-13                            419,424\n",
       "│    │    └─MLP: 3-14                                   132,355\n",
       "================================================================================\n",
       "Total params: 91,350,543\n",
       "Trainable params: 4,071,695\n",
       "Non-trainable params: 87,278,848\n",
       "================================================================================"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "net.load_lora_parameters('/home/manav/SAMed-mod/output/Train100/Pleural-Effusion_512_pretrain_vit_b_epo200_bs2_lr0.005/epoch_159.pth')\n",
    "summary(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab3cf22f-2056-47ce-b3b6-523b891563db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoRA_Sam(\n",
       "  (sam): Sam(\n",
       "    (image_encoder): ImageEncoderViT(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (blocks): ModuleList(\n",
       "        (0-11): 12 x Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): _LoRA_qkv(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (linear_a_q): Linear(in_features=768, out_features=4, bias=False)\n",
       "              (linear_b_q): Linear(in_features=4, out_features=768, bias=False)\n",
       "              (linear_a_v): Linear(in_features=768, out_features=4, bias=False)\n",
       "              (linear_b_v): Linear(in_features=4, out_features=768, bias=False)\n",
       "            )\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (neck): Sequential(\n",
       "        (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): LayerNorm2d()\n",
       "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (3): LayerNorm2d()\n",
       "      )\n",
       "    )\n",
       "    (prompt_encoder): PromptEncoder(\n",
       "      (pe_layer): PositionEmbeddingRandom()\n",
       "      (point_embeddings): ModuleList(\n",
       "        (0-3): 4 x Embedding(1, 256)\n",
       "      )\n",
       "      (not_a_point_embed): Embedding(1, 256)\n",
       "      (mask_downscaling): Sequential(\n",
       "        (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (1): LayerNorm2d()\n",
       "        (2): GELU(approximate='none')\n",
       "        (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (4): LayerNorm2d()\n",
       "        (5): GELU(approximate='none')\n",
       "        (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (no_mask_embed): Embedding(1, 256)\n",
       "    )\n",
       "    (mask_decoder): MaskDecoder(\n",
       "      (transformer): TwoWayTransformer(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x TwoWayAttentionBlock(\n",
       "            (self_attn): Attention(\n",
       "              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (cross_attn_token_to_image): Attention(\n",
       "              (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "            )\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): MLPBlock(\n",
       "              (lin1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "              (lin2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (cross_attn_image_to_token): Attention(\n",
       "              (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_attn_token_to_image): Attention(\n",
       "          (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "          (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "          (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "        )\n",
       "        (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (iou_token): Embedding(1, 256)\n",
       "      (mask_tokens): Embedding(3, 256)\n",
       "      (output_upscaling): Sequential(\n",
       "        (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (1): LayerNorm2d()\n",
       "        (2): GELU(approximate='none')\n",
       "        (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (4): GELU(approximate='none')\n",
       "      )\n",
       "      (output_hypernetworks_mlps): ModuleList(\n",
       "        (0-2): 3 x MLP(\n",
       "          (layers): ModuleList(\n",
       "            (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "            (2): Linear(in_features=256, out_features=32, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (iou_prediction_head): MLP(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "          (2): Linear(in_features=256, out_features=3, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f31468-cc02-4238-9970-7f2ee35b2bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = open(\"/home/manav/SAMed-mod/Pleural-Effusion/lists/test.txt\", \"r\")\n",
    "non_zeros = 0\n",
    "for aline in train_list[:3]:\n",
    "    print(aline.strip())\n",
    "    npzfile = np.load(os.path.join('/home/user1/ChestX-ray/Train/'+ str(aline.strip())+ '.npz'))\n",
    "    image = npzfile['image']\n",
    "    inputs = torch.from_numpy(image).unsqueeze(0).unsqueeze(0).float().cuda()\n",
    "    inputs = repeat(inputs, 'b c h w -> b (repeat c) h w', repeat=3)\n",
    "    outputs = net(inputs, True, 1024)\n",
    "    output_masks = outputs['masks']\n",
    "    out = torch.argmax(torch.softmax(output_masks, dim=1), dim=1).squeeze(0)\n",
    "    if sum(sum(out)) != 0:\n",
    "        non_zeros += 1\n",
    "        print('Non zeros mask :)')\n",
    "print(non_zeros)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
